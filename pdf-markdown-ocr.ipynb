{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mistralai in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: eval-type-backport>=0.2.0 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from mistralai) (0.2.2)\n",
      "Requirement already satisfied: httpx>=0.27.0 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from mistralai) (0.28.1)\n",
      "Requirement already satisfied: jsonpath-python>=1.0.6 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from mistralai) (1.0.6)\n",
      "Requirement already satisfied: pydantic>=2.9.0 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from mistralai) (2.10.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from mistralai) (2.9.0.post0)\n",
      "Requirement already satisfied: typing-inspect>=0.9.0 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from mistralai) (0.9.0)\n",
      "Requirement already satisfied: anyio in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from httpx>=0.27.0->mistralai) (4.8.0)\n",
      "Requirement already satisfied: certifi in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from httpx>=0.27.0->mistralai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from httpx>=0.27.0->mistralai) (1.0.7)\n",
      "Requirement already satisfied: idna in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from httpx>=0.27.0->mistralai) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->mistralai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from pydantic>=2.9.0->mistralai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from pydantic>=2.9.0->mistralai) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from pydantic>=2.9.0->mistralai) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->mistralai) (1.17.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from typing-inspect>=0.9.0->mistralai) (1.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from anyio->httpx>=0.27.0->mistralai) (1.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install mistralai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from mistralai import Mistral, DocumentURLChunk\n",
    "from mistralai.models import OCRResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded API Key: 9ABf...\n"
     ]
    }
   ],
   "source": [
    "# The only requirement for this script is to have a Mistral API Key.\n",
    "# You can get a free API Key at: https://console.mistral.ai/api-keys\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = \"9ABfnEldWDRL5oisq0gY53yvXujV94hX\"\n",
    "print(f\"Loaded API Key: {api_key[:4]}...\")\n",
    "client = Mistral(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path configuration\n",
    "INPUT_DIR = Path(\"./content/pdf_content/\")   # Folder where th9ABfnEldWDRL5oisq0gY53yvXujV94hXe user places the PDFs to be processed\n",
    "DONE_DIR = Path(\"./content/pdf_content/markdown_outputs\")            # Folder where processed PDFs will be moved\n",
    "OUTPUT_ROOT_DIR = Path(\"./content/pdf_content/ocr_output\")    # Root folder for conversion results\n",
    "\n",
    "# Ensure directories exist\n",
    "INPUT_DIR.mkdir(exist_ok=True)\n",
    "DONE_DIR.mkdir(exist_ok=True)\n",
    "OUTPUT_ROOT_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_images_in_markdown(markdown_str: str, images_dict: dict) -> str:\n",
    "    \"\"\"\n",
    "    This converts base64 encoded images directly in the markdown...\n",
    "    And replaces them with links to external images, so the markdown is more readable and organized.\n",
    "    \"\"\"\n",
    "    for img_name, base64_str in images_dict.items():\n",
    "        markdown_str = markdown_str.replace(f\"![{img_name}]({img_name})\", f\"![{img_name}]({base64_str})\")\n",
    "    return markdown_str\n",
    "\n",
    "def get_combined_markdown(ocr_response: OCRResponse) -> str:\n",
    "    \"\"\"\n",
    "    Part of the response from the Mistral API, which is an OCRResponse object...\n",
    "    And returns a single string with the combined markdown of all the pages of the PDF.\n",
    "    \"\"\"\n",
    "    markdowns: list[str] = []\n",
    "    for page in ocr_response.pages:\n",
    "        image_data = {}\n",
    "        for img in page.images:\n",
    "            image_data[img.id] = img.image_base64\n",
    "        markdowns.append(replace_images_in_markdown(page.markdown, image_data))\n",
    "\n",
    "    return \"\\n\\n\".join(markdowns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import base64\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Assuming these globals are defined elsewhere in your project:\n",
    "# OUTPUT_ROOT_DIR, DONE_DIR, client, DocumentURLChunk\n",
    "\n",
    "output_file_relative_path = None\n",
    "output_images_relative_path = None\n",
    "\n",
    "def process_pdf(pdf_path: Path):\n",
    "    global output_file_relative_path, output_images_relative_path\n",
    "    # Process all PDFs in INPUT_DIR\n",
    "    # - Important to be careful with the number of PDFs, as the Mistral API has a usage limit\n",
    "    #   and it could cause errors by exceeding the limit.\n",
    "\n",
    "    # PDF base name\n",
    "    pdf_base = pdf_path.stem\n",
    "    print(f\"Processing {pdf_path.name} ...\")\n",
    "    \n",
    "    # Output folders\n",
    "    output_dir = OUTPUT_ROOT_DIR / pdf_base\n",
    "    output_file_relative_path = output_dir\n",
    "    # Overwrite the output directory if it already exists\n",
    "    if output_dir.exists():\n",
    "        shutil.rmtree(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    images_dir = output_dir / \"images\"\n",
    "    output_images_relative_path = images_dir\n",
    "    images_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # PDF -> OCR\n",
    "    with open(pdf_path, \"rb\") as f:\n",
    "        pdf_bytes = f.read()\n",
    "        \n",
    "    uploaded_file = client.files.upload(\n",
    "        file={\n",
    "            \"file_name\": pdf_path.name,\n",
    "            \"content\": pdf_bytes,\n",
    "        },\n",
    "        purpose=\"ocr\"\n",
    "    )\n",
    "    \n",
    "    signed_url = client.files.get_signed_url(file_id=uploaded_file.id, expiry=1)\n",
    "    \n",
    "    ocr_response = client.ocr.process(\n",
    "        document=DocumentURLChunk(document_url=signed_url.url),\n",
    "        model=\"mistral-ocr-latest\",\n",
    "        include_image_base64=True\n",
    "    )\n",
    "    \n",
    "    # Save OCR in JSON \n",
    "    # (in case something fails it could be reused, but it is not used in the rest of the code)\n",
    "    ocr_json_path = output_dir / \"ocr_response.json\"\n",
    "    with open(ocr_json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(ocr_response.dict(), json_file, indent=4, ensure_ascii=False)\n",
    "    print(f\"OCR response saved in {ocr_json_path}\")\n",
    "    \n",
    "    # OCR -> Markdown prepared for Obsidian\n",
    "    # - That is, from base64 encoded images, it converts them to links to \n",
    "    #   external images and generates the images as such, in a subfolder.\n",
    "    \n",
    "    global_counter = 1\n",
    "    updated_markdown_pages = []\n",
    "    \n",
    "    for page in ocr_response.pages:\n",
    "        updated_markdown = page.markdown\n",
    "        for image_obj in page.images:\n",
    "            \n",
    "            # base64 to image\n",
    "            base64_str = image_obj.image_base64\n",
    "            if base64_str.startswith(\"data:\"):\n",
    "                base64_str = base64_str.split(\",\", 1)[1]\n",
    "            image_bytes = base64.b64decode(base64_str)\n",
    "            \n",
    "            # image extension handling\n",
    "            ext = Path(image_obj.id).suffix if Path(image_obj.id).suffix else \".png\"\n",
    "            new_image_name = f\"{pdf_base}_img_{global_counter}{ext}\"\n",
    "            global_counter += 1\n",
    "            \n",
    "            # save image in subfolder\n",
    "            image_output_path = images_dir / new_image_name\n",
    "            with open(image_output_path, \"wb\") as f:\n",
    "                f.write(image_bytes)\n",
    "            \n",
    "            # Update markdown with wikilink: ![[new_image_name]]\n",
    "            updated_markdown = updated_markdown.replace(\n",
    "                f\"![{image_obj.id}]({image_obj.id})\",\n",
    "                f\"![[{new_image_name}]]\"\n",
    "            )\n",
    "        updated_markdown_pages.append(updated_markdown)\n",
    "    \n",
    "    final_markdown = \"\\n\\n\".join(updated_markdown_pages)\n",
    "    output_markdown_path = output_dir / \"output.md\"\n",
    "    output_file_relative_path = output_markdown_path\n",
    "    with open(output_markdown_path, \"w\", encoding=\"utf-8\") as md_file:\n",
    "        md_file.write(final_markdown)\n",
    "    print(f\"Markdown generated in {output_markdown_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No PDFs to process.\n"
     ]
    }
   ],
   "source": [
    "pdf_files = list(INPUT_DIR.glob(\"*.pdf\"))\n",
    "if not pdf_files:\n",
    "    print(\"No PDFs to process.\")\n",
    "    exit()\n",
    "    \n",
    "for pdf_file in pdf_files:\n",
    "    try:\n",
    "        process_pdf(pdf_file)\n",
    "        shutil.move(str(pdf_file), DONE_DIR / pdf_file.name)\n",
    "        print(f\"{pdf_file.name} moved to {DONE_DIR}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdf_file.name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_images_relative_path, output_file_relative_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No PDFs to process.\n"
     ]
    }
   ],
   "source": [
    "# Process all PDFs in INPUT_DIR\n",
    "# - Important to be careful with the number of PDFs, as the Mistral API has a usage limit\n",
    "#   and it could cause errors by exceeding the limit.\n",
    "\n",
    "pdf_files = list(INPUT_DIR.glob(\"*.pdf\"))\n",
    "if not pdf_files:\n",
    "    print(\"No PDFs to process.\")\n",
    "    exit()\n",
    "    \n",
    "for pdf_file in pdf_files:\n",
    "    try:\n",
    "        process_pdf(pdf_file)\n",
    "        shutil.move(str(pdf_file), DONE_DIR / pdf_file.name)\n",
    "        print(f\"{pdf_file.name} moved to {DONE_DIR}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdf_file.name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'moondream'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmoondream\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmd\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Initialize the Moondream model with your API key\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'moondream'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import moondream as md\n",
    "from PIL import Image\n",
    "\n",
    "# Initialize the Moondream model with your API key\n",
    "model = md.vl(api_key=\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJrZXlfaWQiOiJlODYyMDEzZC00NmVkLTRiNDYtOGMxZi0xYzYwMTUzY2M0YjkiLCJpYXQiOjE3Mzc1MjYyMjd9.0agZ8vgxwgrUJ7YMrIoBqGPs_4hsuh2zhqkwckxYkIM\")\n",
    "\n",
    "def generate_description_for_image(image_path, figure_caption=\"\"):\n",
    "    \"\"\"\n",
    "    Load an image from the provided path, encode it using the Moondream API,\n",
    "    and query for a description that is based on the provided figure caption.\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path)\n",
    "    encoded_image = model.encode_image(image)\n",
    "    query_text = (\n",
    "        f\"Describe the key technical findings in this figure/visualization \"\n",
    "        f\"captioned: {figure_caption} using natural language. Illustrate and mention trends, \"\n",
    "        f\"patterns, and numerical values that can be observed. Provide a scientific/academic styled short, \"\n",
    "        f\"single paragraph summary that is highly insightful in context of the document.\"\n",
    "    )\n",
    "    response = model.query(encoded_image, query_text)\n",
    "    description = response.get(\"answer\", \"No description available.\")\n",
    "    return description\n",
    "\n",
    "def extract_captions_from_markdown(markdown_path):\n",
    "    \"\"\"\n",
    "    Parse the markdown file to build a mapping from image filename to its figure caption.\n",
    "    It looks for placeholders like ![[filename]] and if the next line starts with \"Figure\",\n",
    "    uses that as the caption.\n",
    "    \"\"\"\n",
    "    captions = {}\n",
    "    with open(markdown_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        line = lines[i]\n",
    "        placeholder_match = re.search(r'!\\[\\[(.*?)\\]\\]', line)\n",
    "        if placeholder_match:\n",
    "            image_filename = placeholder_match.group(1)\n",
    "            caption = \"\"\n",
    "            # Check if the following line is a figure caption\n",
    "            if i + 1 < len(lines) and lines[i + 1].strip().startswith(\"Figure\"):\n",
    "                caption = lines[i + 1].strip()\n",
    "            captions[image_filename] = caption\n",
    "        i += 1\n",
    "    return captions\n",
    "\n",
    "def generate_image_descriptions(images_folder, captions_mapping):\n",
    "    \"\"\"\n",
    "    For each image file (as referenced by the markdown file), load the image from the\n",
    "    given images folder and generate a description using the Moondream API.\n",
    "    \"\"\"\n",
    "    descriptions = {}\n",
    "    for image_filename, caption in captions_mapping.items():\n",
    "        image_path = os.path.join(images_folder, image_filename)\n",
    "        if os.path.exists(image_path):\n",
    "            print(f\"Processing image: {image_filename}\")\n",
    "            description = generate_description_for_image(image_path, caption)\n",
    "        else:\n",
    "            description = \"Image file not found.\"\n",
    "        descriptions[image_filename] = description\n",
    "    return descriptions\n",
    "\n",
    "def update_markdown_file(markdown_path, image_descriptions):\n",
    "    \"\"\"\n",
    "    Update the markdown file by replacing the image placeholder (and the adjacent figure\n",
    "    caption) with a markdown block that includes the original caption and the generated\n",
    "    image description.\n",
    "    \"\"\"\n",
    "    with open(markdown_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    updated_lines = []\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        line = lines[i]\n",
    "        placeholder_match = re.search(r'!\\[\\[(.*?)\\]\\]', line)\n",
    "        if placeholder_match:\n",
    "            image_filename = placeholder_match.group(1)\n",
    "            caption = \"\"\n",
    "            # If the next line is a figure caption, capture it and skip it in the output.\n",
    "            if i + 1 < len(lines) and lines[i + 1].strip().startswith(\"Figure\"):\n",
    "                caption = lines[i + 1].strip()\n",
    "                i += 1  # Skip the caption line since we'll include it in our replacement.\n",
    "            description = image_descriptions.get(image_filename, \"No description available.\")\n",
    "            replacement = f\"{caption}\\n\\n**Image Description:** {description}\\n\"\n",
    "            updated_lines.append(replacement)\n",
    "        else:\n",
    "            updated_lines.append(line)\n",
    "        i += 1\n",
    "\n",
    "    with open(markdown_path, 'w', encoding='utf-8') as f:\n",
    "        f.writelines(updated_lines)\n",
    "\n",
    "def main():\n",
    "    # Define paths for images and markdown file.\n",
    "    global output_file_relative_path, output_images_relative_path\n",
    "    images_folder = str(output_images_relative_path)   # Update this path if your images are elsewhere.\n",
    "    markdown_file = str(output_file_relative_path) # Update this path if your markdown file is elsewhere.\n",
    "    \n",
    "    # First, extract the figure captions from the markdown file.\n",
    "    captions_mapping = extract_captions_from_markdown(markdown_file)\n",
    "    \n",
    "    # Next, generate image descriptions using the Moondream API.\n",
    "    image_descriptions = generate_image_descriptions(images_folder, captions_mapping)\n",
    "    \n",
    "    # Finally, update the markdown file by replacing placeholders with the descriptions.\n",
    "    update_markdown_file(markdown_file, image_descriptions)\n",
    "    \n",
    "    print(\"Markdown file updated with image descriptions.\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image: DeepSeek_R1_img_1.jpeg\n",
      "Processing image: DeepSeek_R1_img_2.jpeg\n",
      "Processing image: DeepSeek_R1_img_3.jpeg\n",
      "Markdown file updated with image descriptions.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "output_file_relative_path = None\n",
    "output_images_relative_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_file_relative_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 108\u001b[39m\n\u001b[32m    105\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDITA XML has been written to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mglobal\u001b[39;00m output_file_relative_path, output_images_relative_path\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m input_md_file = \u001b[38;5;28mstr\u001b[39m(\u001b[43moutput_file_relative_path\u001b[49m)  \u001b[38;5;66;03m# Your OCR-generated markdown file\u001b[39;00m\n\u001b[32m    109\u001b[39m output_dita_file =  \u001b[38;5;28mstr\u001b[39m(output_images_relative_path) \u001b[38;5;66;03m# Output file for DITA XML\u001b[39;00m\n\u001b[32m    110\u001b[39m convert_markdown_file_to_dita(input_md_file, output_dita_file)\n",
      "\u001b[31mNameError\u001b[39m: name 'output_file_relative_path' is not defined"
     ]
    }
   ],
   "source": [
    "# import re\n",
    "# import xml.etree.ElementTree as ET\n",
    "\n",
    "# def convert_latex_to_dita(text):\n",
    "#     \"\"\"\n",
    "#     Convert LaTeX math in the markdown to DITA-friendly <formula> tags.\n",
    "#     - Display math: $$...$$ becomes a formula block with inline=\"false\"\n",
    "#     - Inline math: $...$ becomes a formula with inline=\"true\"\n",
    "#     \"\"\"\n",
    "#     # Replace display math blocks (using DOTALL so newlines are allowed)\n",
    "#     text = re.sub(r'\\$\\$(.+?)\\$\\$', r'<formula inline=\"false\">\\1</formula>', text, flags=re.DOTALL)\n",
    "#     # Replace inline math\n",
    "#     text = re.sub(r'\\$(.+?)\\$', r'<formula inline=\"true\">\\1</formula>', text)\n",
    "#     return text\n",
    "\n",
    "# def parse_markdown_to_chunks(md_text):\n",
    "#     \"\"\"\n",
    "#     Parse the preprocessed markdown text into a hierarchical chunk tree.\n",
    "#     Each chunk is a dict with:\n",
    "#       - level: heading level (0 for root)\n",
    "#       - title: heading text\n",
    "#       - content: list of text lines not recognized as headings\n",
    "#       - children: list of sub-chunks\n",
    "#     \"\"\"\n",
    "#     lines = md_text.splitlines()\n",
    "#     # Create a dummy root chunk with level 0.\n",
    "#     root = {\"level\": 0, \"title\": \"Root\", \"content\": [], \"children\": []}\n",
    "#     current_section = root\n",
    "#     section_stack = [root]\n",
    "    \n",
    "#     for line in lines:\n",
    "#         # Look for markdown headings (e.g., \"#\", \"##\", etc.)\n",
    "#         heading_match = re.match(r'^(#+)\\s*(.*)', line)\n",
    "#         if heading_match:\n",
    "#             level = len(heading_match.group(1))\n",
    "#             title = heading_match.group(2).strip()\n",
    "#             # Create a new section dictionary for this heading.\n",
    "#             new_section = {\"level\": level, \"title\": title, \"content\": [], \"children\": []}\n",
    "#             # Pop sections until we find the correct parent (lower level)\n",
    "#             while section_stack and section_stack[-1][\"level\"] >= level:\n",
    "#                 section_stack.pop()\n",
    "#             # The current top of the stack is the parent section.\n",
    "#             parent = section_stack[-1]\n",
    "#             parent[\"children\"].append(new_section)\n",
    "#             section_stack.append(new_section)\n",
    "#             current_section = new_section\n",
    "#         else:\n",
    "#             # Add non-heading lines to the current section's content.\n",
    "#             current_section[\"content\"].append(line)\n",
    "#     return root\n",
    "\n",
    "# def build_dita_xml(section):\n",
    "#     \"\"\"\n",
    "#     Recursively convert the chunk tree into a DITA XML structure.\n",
    "#     The root (level 0) becomes the <topic> element, and each subsequent chunk becomes a <section>.\n",
    "#     \"\"\"\n",
    "#     if section[\"level\"] == 0:\n",
    "#         # For the root, create a topic element.\n",
    "#         topic = ET.Element(\"topic\", {\"id\": \"root\"})\n",
    "#         title_el = ET.SubElement(topic, \"title\")\n",
    "#         title_el.text = \"Converted Lecture Notes\"\n",
    "#         body_el = ET.SubElement(topic, \"body\")\n",
    "#         # Process each child of the root as a section.\n",
    "#         for child in section[\"children\"]:\n",
    "#             body_el.append(build_dita_xml(child))\n",
    "#         return topic\n",
    "#     else:\n",
    "#         # For any section, create a <section> element.\n",
    "#         sec = ET.Element(\"section\")\n",
    "#         title_el = ET.SubElement(sec, \"title\")\n",
    "#         title_el.text = section[\"title\"]\n",
    "        \n",
    "#         # Join content lines and split by double newlines to form paragraphs.\n",
    "#         content_text = \"\\n\".join(section[\"content\"]).strip()\n",
    "#         for para in content_text.split(\"\\n\\n\"):\n",
    "#             para = para.strip()\n",
    "#             if para:\n",
    "#                 p_el = ET.SubElement(sec, \"p\")\n",
    "#                 p_el.text = para\n",
    "#         # Recursively add any sub-sections.\n",
    "#         for child in section[\"children\"]:\n",
    "#             sec.append(build_dita_xml(child))\n",
    "#         return sec\n",
    "\n",
    "# def convert_markdown_file_to_dita(input_path, output_path):\n",
    "#     \"\"\"\n",
    "#     Main function to convert a markdown file (with LaTeX) into a DITA XML file.\n",
    "#     \"\"\"\n",
    "#     # Read the markdown file\n",
    "#     with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         md_text = f.read()\n",
    "    \n",
    "#     # Preprocess the LaTeX formulas in the text.\n",
    "#     processed_text = convert_latex_to_dita(md_text)\n",
    "    \n",
    "#     # Parse the processed markdown into hierarchical chunks.\n",
    "#     chunks = parse_markdown_to_chunks(processed_text)\n",
    "    \n",
    "#     # Build a DITA XML tree from the hierarchical chunks.\n",
    "#     dita_xml = build_dita_xml(chunks)\n",
    "    \n",
    "#     # Write the resulting XML tree to an output file.\n",
    "#     tree = ET.ElementTree(dita_xml)\n",
    "#     tree.write(output_path, encoding=\"utf-8\", xml_declaration=True)\n",
    "#     print(f\"DITA XML has been written to {output_path}\")\n",
    "\n",
    "# global output_file_relative_path, output_images_relative_path\n",
    "# input_md_file = str(output_file_relative_path)  # Your OCR-generated markdown file\n",
    "# output_dita_file =  str(output_images_relative_path) # Output file for DITA XML\n",
    "# convert_markdown_file_to_dita(input_md_file, output_dita_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
