{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mistralai in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: eval-type-backport>=0.2.0 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from mistralai) (0.2.2)\n",
      "Requirement already satisfied: httpx>=0.27.0 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from mistralai) (0.28.1)\n",
      "Requirement already satisfied: jsonpath-python>=1.0.6 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from mistralai) (1.0.6)\n",
      "Requirement already satisfied: pydantic>=2.9.0 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from mistralai) (2.10.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from mistralai) (2.9.0.post0)\n",
      "Requirement already satisfied: typing-inspect>=0.9.0 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from mistralai) (0.9.0)\n",
      "Requirement already satisfied: anyio in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from httpx>=0.27.0->mistralai) (4.8.0)\n",
      "Requirement already satisfied: certifi in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from httpx>=0.27.0->mistralai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from httpx>=0.27.0->mistralai) (1.0.7)\n",
      "Requirement already satisfied: idna in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from httpx>=0.27.0->mistralai) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->mistralai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from pydantic>=2.9.0->mistralai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from pydantic>=2.9.0->mistralai) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from pydantic>=2.9.0->mistralai) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->mistralai) (1.17.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from typing-inspect>=0.9.0->mistralai) (1.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from anyio->httpx>=0.27.0->mistralai) (1.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (0.3.20)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.41 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from langchain) (0.3.44)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from langchain) (0.3.6)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from langchain) (0.3.13)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from langchain) (2.10.6)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from langchain) (2.0.39)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: anyio in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain) (3.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install mistralai\n",
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from mistralai import Mistral, DocumentURLChunk\n",
    "from mistralai.models import OCRResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded API Key: 9ABf...\n"
     ]
    }
   ],
   "source": [
    "# The only requirement for this script is to have a Mistral API Key.\n",
    "# You can get a free API Key at: https://console.mistral.ai/api-keys\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = \"9ABfnEldWDRL5oisq0gY53yvXujV94hX\"\n",
    "print(f\"Loaded API Key: {api_key[:4]}...\")\n",
    "client = Mistral(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path configuration\n",
    "INPUT_DIR = Path(\"./content/pdf_content/\")   # Folder where th9ABfnEldWDRL5oisq0gY53yvXujV94hXe user places the PDFs to be processed\n",
    "DONE_DIR = Path(\"./content/pdf_content/markdown_outputs\")            # Folder where processed PDFs will be moved\n",
    "OUTPUT_ROOT_DIR = Path(\"./content/pdf_content/ocr_output\")    # Root folder for conversion results\n",
    "\n",
    "# Ensure directories exist\n",
    "INPUT_DIR.mkdir(exist_ok=True)\n",
    "DONE_DIR.mkdir(exist_ok=True)\n",
    "OUTPUT_ROOT_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_images_in_markdown(markdown_str: str, images_dict: dict) -> str:\n",
    "    \"\"\"\n",
    "    This converts base64 encoded images directly in the markdown...\n",
    "    And replaces them with links to external images, so the markdown is more readable and organized.\n",
    "    \"\"\"\n",
    "    for img_name, base64_str in images_dict.items():\n",
    "        markdown_str = markdown_str.replace(f\"![{img_name}]({img_name})\", f\"![{img_name}]({base64_str})\")\n",
    "    return markdown_str\n",
    "\n",
    "def get_combined_markdown(ocr_response: OCRResponse) -> str:\n",
    "    \"\"\"\n",
    "    Part of the response from the Mistral API, which is an OCRResponse object...\n",
    "    And returns a single string with the combined markdown of all the pages of the PDF.\n",
    "    \"\"\"\n",
    "    markdowns: list[str] = []\n",
    "    for page in ocr_response.pages:\n",
    "        image_data = {}\n",
    "        for img in page.images:\n",
    "            image_data[img.id] = img.image_base64\n",
    "        markdowns.append(replace_images_in_markdown(page.markdown, image_data))\n",
    "\n",
    "    return \"\\n\\n\".join(markdowns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import base64\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Assuming these globals are defined elsewhere in your project:\n",
    "# OUTPUT_ROOT_DIR, DONE_DIR, client, DocumentURLChunk\n",
    "\n",
    "output_file_relative_path = None\n",
    "output_images_relative_path = None\n",
    "\n",
    "def process_pdf(pdf_path: Path):\n",
    "    global output_file_relative_path, output_images_relative_path\n",
    "    # Process all PDFs in INPUT_DIR\n",
    "    # - Important to be careful with the number of PDFs, as the Mistral API has a usage limit\n",
    "    #   and it could cause errors by exceeding the limit.\n",
    "\n",
    "    # PDF base name\n",
    "    pdf_base = pdf_path.stem\n",
    "    print(f\"Processing {pdf_path.name} ...\")\n",
    "    \n",
    "    # Output folders\n",
    "    output_dir = OUTPUT_ROOT_DIR / pdf_base\n",
    "    output_file_relative_path = output_dir\n",
    "    # Overwrite the output directory if it already exists\n",
    "    if output_dir.exists():\n",
    "        shutil.rmtree(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    images_dir = output_dir / \"images\"\n",
    "    output_images_relative_path = images_dir\n",
    "    images_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # PDF -> OCR\n",
    "    with open(pdf_path, \"rb\") as f:\n",
    "        pdf_bytes = f.read()\n",
    "        \n",
    "    uploaded_file = client.files.upload(\n",
    "        file={\n",
    "            \"file_name\": pdf_path.name,\n",
    "            \"content\": pdf_bytes,\n",
    "        },\n",
    "        purpose=\"ocr\"\n",
    "    )\n",
    "    \n",
    "    signed_url = client.files.get_signed_url(file_id=uploaded_file.id, expiry=1)\n",
    "    \n",
    "    ocr_response = client.ocr.process(\n",
    "        document=DocumentURLChunk(document_url=signed_url.url),\n",
    "        model=\"mistral-ocr-latest\",\n",
    "        include_image_base64=True\n",
    "    )\n",
    "    \n",
    "    # Save OCR in JSON \n",
    "    # (in case something fails it could be reused, but it is not used in the rest of the code)\n",
    "    ocr_json_path = output_dir / \"ocr_response.json\"\n",
    "    with open(ocr_json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(ocr_response.dict(), json_file, indent=4, ensure_ascii=False)\n",
    "    print(f\"OCR response saved in {ocr_json_path}\")\n",
    "    \n",
    "    # OCR -> Markdown prepared for Obsidian\n",
    "    # - That is, from base64 encoded images, it converts them to links to \n",
    "    #   external images and generates the images as such, in a subfolder.\n",
    "    \n",
    "    global_counter = 1\n",
    "    updated_markdown_pages = []\n",
    "    \n",
    "    for page in ocr_response.pages:\n",
    "        updated_markdown = page.markdown\n",
    "        for image_obj in page.images:\n",
    "            \n",
    "            # base64 to image\n",
    "            base64_str = image_obj.image_base64\n",
    "            if base64_str.startswith(\"data:\"):\n",
    "                base64_str = base64_str.split(\",\", 1)[1]\n",
    "            image_bytes = base64.b64decode(base64_str)\n",
    "            \n",
    "            # image extension handling\n",
    "            ext = Path(image_obj.id).suffix if Path(image_obj.id).suffix else \".png\"\n",
    "            new_image_name = f\"{pdf_base}_img_{global_counter}{ext}\"\n",
    "            global_counter += 1\n",
    "            \n",
    "            # save image in subfolder\n",
    "            image_output_path = images_dir / new_image_name\n",
    "            with open(image_output_path, \"wb\") as f:\n",
    "                f.write(image_bytes)\n",
    "            \n",
    "            # Update markdown with wikilink: ![[new_image_name]]\n",
    "            updated_markdown = updated_markdown.replace(\n",
    "                f\"![{image_obj.id}]({image_obj.id})\",\n",
    "                f\"![[{new_image_name}]]\"\n",
    "            )\n",
    "        updated_markdown_pages.append(updated_markdown)\n",
    "    \n",
    "    final_markdown = \"\\n\\n\".join(updated_markdown_pages)\n",
    "    output_markdown_path = output_dir / \"output.md\"\n",
    "    output_file_relative_path = output_markdown_path\n",
    "    with open(output_markdown_path, \"w\", encoding=\"utf-8\") as md_file:\n",
    "        md_file.write(final_markdown)\n",
    "    print(f\"Markdown generated in {output_markdown_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing BFS_notespdf.pdf ...\n",
      "OCR response saved in content\\pdf_content\\ocr_output\\BFS_notespdf\\ocr_response.json\n",
      "Markdown generated in content\\pdf_content\\ocr_output\\BFS_notespdf\\output.md\n",
      "BFS_notespdf.pdf moved to content\\pdf_content\\markdown_outputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anandaraman\\AppData\\Local\\Temp\\ipykernel_5700\\2116534252.py:60: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  json.dump(ocr_response.dict(), json_file, indent=4, ensure_ascii=False)\n"
     ]
    }
   ],
   "source": [
    "pdf_files = list(INPUT_DIR.glob(\"*.pdf\"))\n",
    "if not pdf_files:\n",
    "    print(\"No PDFs to process.\")\n",
    "    exit()\n",
    "    \n",
    "for pdf_file in pdf_files:\n",
    "    try:\n",
    "        process_pdf(pdf_file)\n",
    "        shutil.move(str(pdf_file), DONE_DIR / pdf_file.name)\n",
    "        print(f\"{pdf_file.name} moved to {DONE_DIR}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdf_file.name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(WindowsPath('content/pdf_content/ocr_output/BFS_notespdf/images'),\n",
       " WindowsPath('content/pdf_content/ocr_output/BFS_notespdf/output.md'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_images_relative_path, output_file_relative_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No PDFs to process.\n"
     ]
    }
   ],
   "source": [
    "# Process all PDFs in INPUT_DIR\n",
    "# - Important to be careful with the number of PDFs, as the Mistral API has a usage limit\n",
    "#   and it could cause errors by exceeding the limit.\n",
    "\n",
    "pdf_files = list(INPUT_DIR.glob(\"*.pdf\"))\n",
    "if not pdf_files:\n",
    "    print(\"No PDFs to process.\")\n",
    "    exit()\n",
    "    \n",
    "for pdf_file in pdf_files:\n",
    "    try:\n",
    "        process_pdf(pdf_file)\n",
    "        shutil.move(str(pdf_file), DONE_DIR / pdf_file.name)\n",
    "        print(f\"{pdf_file.name} moved to {DONE_DIR}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdf_file.name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image: BFS_notespdf_img_1.jpeg\n",
      "Processing image: BFS_notespdf_img_2.jpeg\n",
      "Processing image: BFS_notespdf_img_3.jpeg\n",
      "Processing image: BFS_notespdf_img_4.jpeg\n",
      "Processing image: BFS_notespdf_img_5.jpeg\n",
      "Processing image: BFS_notespdf_img_6.jpeg\n",
      "Processing image: BFS_notespdf_img_7.jpeg\n",
      "Processing image: BFS_notespdf_img_8.jpeg\n",
      "Markdown file updated with image descriptions.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import moondream as md\n",
    "from PIL import Image\n",
    "\n",
    "# Initialize the Moondream model with your API key\n",
    "model = md.vl(api_key=\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJrZXlfaWQiOiJlODYyMDEzZC00NmVkLTRiNDYtOGMxZi0xYzYwMTUzY2M0YjkiLCJpYXQiOjE3Mzc1MjYyMjd9.0agZ8vgxwgrUJ7YMrIoBqGPs_4hsuh2zhqkwckxYkIM\")\n",
    "\n",
    "def generate_description_for_image(image_path, figure_caption=\"\"):\n",
    "    \"\"\"\n",
    "    Load an image from the provided path, encode it using the Moondream API,\n",
    "    and query for a description that is based on the provided figure caption.\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path)\n",
    "    encoded_image = model.encode_image(image)\n",
    "    query_text = (\n",
    "        f\"Describe the key technical findings in this figure/visualization \"\n",
    "        f\"captioned: {figure_caption} using natural language. Illustrate and mention trends, \"\n",
    "        f\"patterns, and numerical values that can be observed. Provide a scientific/academic styled short, \"\n",
    "        f\"single paragraph summary that is highly insightful in context of the document.\"\n",
    "    )\n",
    "    response = model.query(encoded_image, query_text)\n",
    "    description = response.get(\"answer\", \"No description available.\")\n",
    "    return description\n",
    "\n",
    "def extract_captions_from_markdown(markdown_path):\n",
    "    \"\"\"\n",
    "    Parse the markdown file to build a mapping from image filename to its figure caption.\n",
    "    It looks for placeholders like ![[filename]] and if the next line starts with \"Figure\",\n",
    "    uses that as the caption.\n",
    "    \"\"\"\n",
    "    captions = {}\n",
    "    with open(markdown_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        line = lines[i]\n",
    "        placeholder_match = re.search(r'!\\[\\[(.*?)\\]\\]', line)\n",
    "        if placeholder_match:\n",
    "            image_filename = placeholder_match.group(1)\n",
    "            caption = \"\"\n",
    "            # Check if the following line is a figure caption\n",
    "            if i + 1 < len(lines) and lines[i + 1].strip().startswith(\"Figure\"):\n",
    "                caption = lines[i + 1].strip()\n",
    "            captions[image_filename] = caption\n",
    "        i += 1\n",
    "    return captions\n",
    "\n",
    "def generate_image_descriptions(images_folder, captions_mapping):\n",
    "    \"\"\"\n",
    "    For each image file (as referenced by the markdown file), load the image from the\n",
    "    given images folder and generate a description using the Moondream API.\n",
    "    \"\"\"\n",
    "    descriptions = {}\n",
    "    for image_filename, caption in captions_mapping.items():\n",
    "        image_path = os.path.join(images_folder, image_filename)\n",
    "        if os.path.exists(image_path):\n",
    "            print(f\"Processing image: {image_filename}\")\n",
    "            description = generate_description_for_image(image_path, caption)\n",
    "        else:\n",
    "            description = \"Image file not found.\"\n",
    "        descriptions[image_filename] = description\n",
    "    return descriptions\n",
    "\n",
    "def update_markdown_file(markdown_path, image_descriptions):\n",
    "    \"\"\"\n",
    "    Update the markdown file by replacing the image placeholder (and the adjacent figure\n",
    "    caption) with a markdown block that includes the original caption and the generated\n",
    "    image description.\n",
    "    \"\"\"\n",
    "    with open(markdown_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    updated_lines = []\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        line = lines[i]\n",
    "        placeholder_match = re.search(r'!\\[\\[(.*?)\\]\\]', line)\n",
    "        if placeholder_match:\n",
    "            image_filename = placeholder_match.group(1)\n",
    "            caption = \"\"\n",
    "            # If the next line is a figure caption, capture it and skip it in the output.\n",
    "            if i + 1 < len(lines) and lines[i + 1].strip().startswith(\"Figure\"):\n",
    "                caption = lines[i + 1].strip()\n",
    "                i += 1  # Skip the caption line since we'll include it in our replacement.\n",
    "            description = image_descriptions.get(image_filename, \"No description available.\")\n",
    "            replacement = f\"{caption}\\n\\n**Image Description:** {description}\\n\"\n",
    "            updated_lines.append(replacement)\n",
    "        else:\n",
    "            updated_lines.append(line)\n",
    "        i += 1\n",
    "\n",
    "    with open(markdown_path, 'w', encoding='utf-8') as f:\n",
    "        f.writelines(updated_lines)\n",
    "\n",
    "\n",
    "    # Define paths for images and markdown file.\n",
    "global output_file_relative_path, output_images_relative_path\n",
    "images_folder = str(output_images_relative_path)   # Update this path if your images are elsewhere.\n",
    "markdown_file = str(output_file_relative_path) # Update this path if your markdown file is elsewhere.\n",
    "    \n",
    "    # First, extract the figure captions from the markdown file.\n",
    "captions_mapping = extract_captions_from_markdown(markdown_file)\n",
    "    \n",
    "    # Next, generate image descriptions using the Moondream API.\n",
    "image_descriptions = generate_image_descriptions(images_folder, captions_mapping)\n",
    "    \n",
    "    # Finally, update the markdown file by replacing placeholders with the descriptions.\n",
    "update_markdown_file(markdown_file, image_descriptions)\n",
    "    \n",
    "print(\"Markdown file updated with image descriptions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (0.3.20)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.41 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from langchain) (0.3.44)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from langchain) (0.3.6)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from langchain) (0.3.13)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from langchain) (2.10.6)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from langchain) (2.0.39)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: anyio in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain) (3.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-community in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (0.3.19)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.41 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from langchain-community) (0.3.44)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.20 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from langchain-community) (0.3.20)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from langchain-community) (2.0.39)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from langchain-community) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from langchain-community) (3.11.13)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from langchain-community) (9.0.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from langchain-community) (2.8.1)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from langchain-community) (0.3.13)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from langchain-community) (2.2.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from langchain<1.0.0,>=0.3.20->langchain-community) (0.3.6)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from langchain<1.0.0,>=0.3.20->langchain-community) (2.10.6)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.41->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.41->langchain-community) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.41->langchain-community) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
      "Requirement already satisfied: anyio in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.20->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.20->langchain-community) (2.27.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.66.2-py3-none-any.whl (567 kB)\n",
      "     -------------------------------------- 567.3/567.3 kB 7.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from openai) (4.8.0)\n",
      "Collecting distro<2,>=1.7.0\n",
      "  Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from openai) (0.28.1)\n",
      "Collecting jiter<1,>=0.4.0\n",
      "  Downloading jiter-0.9.0-cp311-cp311-win_amd64.whl (210 kB)\n",
      "     ------------------------------------- 210.1/210.1 kB 12.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from openai) (2.10.6)\n",
      "Requirement already satisfied: sniffio in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
      "Requirement already satisfied: colorama in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Installing collected packages: jiter, distro, openai\n",
      "Successfully installed distro-1.9.0 jiter-0.9.0 openai-1.66.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "C:\\Users\\Anandaraman\\AppData\\Local\\Temp\\ipykernel_7984\\570330778.py:52: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent = initialize_agent(\n"
     ]
    }
   ],
   "source": [
    "# !pip install langchain\n",
    "# !pip install langchain-community\n",
    "# !pip install openai\n",
    "\n",
    "# import os\n",
    "# import re\n",
    "# from pathlib import Path\n",
    "# from langchain.agents import Tool, initialize_agent\n",
    "# from langchain.llms import OpenAI\n",
    "\n",
    "# # Set OpenAI API Key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"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\"  # Replace with your actual key\n",
    "# output_file_relative_path = \"content\\pdf_content\\ocr_output\\BFS_notespdf\\output.md\"\n",
    "\n",
    "# # Function to process LaTeX and non-markdown symbols\n",
    "# def convert_markdown(md_content: str) -> str:\n",
    "#     \"\"\"Cleans markdown content by replacing LaTeX expressions and non-markdown symbols.\"\"\"\n",
    "\n",
    "#     # Normalize inline math expressions (ensure spaces inside `$...$` math expressions)\n",
    "#     def fix_inline_math(match):\n",
    "#         content = match.group(1).strip()\n",
    "#         return f\"$ {content} $\"\n",
    "    \n",
    "#     md_content = re.sub(r\"\\$(.+?)\\$\", fix_inline_math, md_content)\n",
    "    \n",
    "#     # Replace common LaTeX symbols with markdown-friendly alternatives\n",
    "#     latex_replacements = {\n",
    "#         r\"\\Longrightarrow\": \"→\",\n",
    "#         r\"\\Theta\": \"Θ\",\n",
    "#         r\"\\cdot\": \"·\",\n",
    "#         # Add more replacements if needed\n",
    "#     }\n",
    "    \n",
    "#     for latex_cmd, replacement in latex_replacements.items():\n",
    "#         md_content = md_content.replace(latex_cmd, replacement)\n",
    "    \n",
    "#     return md_content\n",
    "\n",
    "# def parse_markdown_tool(input_text: str) -> str:\n",
    "#     \"\"\"Tool that converts a given markdown text into a cleaned markdown format.\"\"\"\n",
    "#     return convert_markdown(input_text)\n",
    "\n",
    "# # Define the LangChain tool\n",
    "# markdown_parser_tool = Tool(\n",
    "#     name=\"MarkdownParser\",\n",
    "#     func=parse_markdown_tool,\n",
    "#     description=\"Scans markdown content for LaTeX or other non-markdown symbols and converts them into proper markdown format suitable for hierarchical chunking.\"\n",
    "# )\n",
    "\n",
    "# # Initialize LangChain LLM agent\n",
    "# llm = OpenAI(temperature=0)\n",
    "# agent = initialize_agent(\n",
    "#     tools=[markdown_parser_tool],\n",
    "#     llm=llm,\n",
    "#     agent=\"zero-shot-react-description\",\n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "# # Directory where the markdown files are stored\n",
    "# input_directory = Path(str(output_file_relative_path))  # Change this to your actual directory\n",
    "\n",
    "# # Process all markdown files in the directory and overwrite them\n",
    "# for md_file in input_directory.glob(\"*.md\"):\n",
    "#     with md_file.open(\"r\", encoding=\"utf-8\") as file:\n",
    "#         content = file.read()\n",
    "    \n",
    "#     # Use the agent to process the markdown content\n",
    "#     converted_content = agent.run(content)\n",
    "    \n",
    "#     # Overwrite the same file with the cleaned markdown content\n",
    "#     with md_file.open(\"w\", encoding=\"utf-8\") as file:\n",
    "#         file.write(converted_content)\n",
    "    \n",
    "#     print(f\"Processed and updated: {md_file.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing markdown files from ./content/pdf_content/ocr_output/BFS_notespdf to ./cleaned_md_outputs\n",
      "Found 1 markdown files\n",
      "Successfully converted content\\pdf_content\\ocr_output\\BFS_notespdf\\output.md to cleaned_md_outputs\\output.md\n",
      "Successfully converted 1 out of 1 files\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def convert_latex_to_markdown(text):\n",
    "    \"\"\"\n",
    "    Convert LaTeX notation to markdown equivalent symbols.\n",
    "    Skip checking or modifying image descriptions and figure descriptions.\n",
    "    \"\"\"\n",
    "    # Identify image and figure descriptions for selective processing\n",
    "    parts = []\n",
    "    last_end = 0\n",
    "    \n",
    "    # Pattern to match both image and figure descriptions\n",
    "    combined_pattern = r'(\\*\\*Image Description:\\*\\*.*?(?=\\n\\n|\\Z))|(Figure \\d+:.*?(?=\\n\\n|\\Z))'\n",
    "    \n",
    "    for match in re.finditer(combined_pattern, text, flags=re.DOTALL):\n",
    "        # Add the text before the match (to be processed)\n",
    "        if match.start() > last_end:\n",
    "            parts.append(('process', text[last_end:match.start()]))\n",
    "        \n",
    "        # Add the match itself (to be preserved as-is)\n",
    "        parts.append(('preserve', match.group(0)))\n",
    "        last_end = match.end()\n",
    "    \n",
    "    # Add any remaining text after the last match\n",
    "    if last_end < len(text):\n",
    "        parts.append(('process', text[last_end:]))\n",
    "    \n",
    "    # Process each part according to its type\n",
    "    processed_parts = []\n",
    "    for part_type, part_text in parts:\n",
    "        if part_type == 'preserve':\n",
    "            # Keep this part as-is\n",
    "            processed_parts.append(part_text)\n",
    "        else:\n",
    "            # Process this part\n",
    "            processed_parts.append(process_latex_part(part_text))\n",
    "    \n",
    "    # Combine all processed parts\n",
    "    return ''.join(processed_parts)\n",
    "\n",
    "def process_latex_part(text):\n",
    "    \"\"\"\n",
    "    Process a part of the text that should have LaTeX converted to markdown.\n",
    "    \"\"\"\n",
    "    # Dictionary of LaTeX to markdown symbol conversions\n",
    "    latex_to_markdown = {\n",
    "        # Operators\n",
    "        r'\\\\times': '×',\n",
    "        r'\\\\div': '÷',\n",
    "        r'\\\\dfrac\\{([^}]*)\\}\\{([^}]*)\\}': r'\\1/\\2',  # Simple fraction replacement\n",
    "        r'\\\\frac\\{([^}]*)\\}\\{([^}]*)\\}': r'\\1/\\2',  # Add standard fraction\n",
    "        r'\\\\sqrt\\{([^}]*)\\}': r'√\\1',\n",
    "        \n",
    "        # Math environments\n",
    "        r'\\\\begin\\{aligned\\}(.*?)\\\\end\\{aligned\\}': r'```math\\n\\1\\n```',\n",
    "        \n",
    "        # Special characters and formatting\n",
    "        r'\\\\textbf\\{([^}]*)\\}': r'**\\1**',  # Bold text\n",
    "        r'\\\\textit\\{([^}]*)\\}': r'*\\1*',    # Italic text\n",
    "        r'\\\\emph\\{([^}]*)\\}': r'*\\1*',      # Emphasized text\n",
    "        r'\\\\underline\\{([^}]*)\\}': r'_\\1_', # Underlined text\n",
    "        \n",
    "        # Common math operations\n",
    "        r'\\\\sum': '∑',\n",
    "        r'\\\\prod': '∏',\n",
    "        r'\\\\int': '∫',\n",
    "        \n",
    "        # Symbols\n",
    "        r'\\\\pi': 'π',\n",
    "        r'\\\\approx': '≈',\n",
    "        r'\\\\pm': '±',\n",
    "        r'\\\\neq': '≠',\n",
    "        r'\\\\infty': '∞',\n",
    "        r'\\\\in': '∈',\n",
    "        r'\\\\notin': '∉',\n",
    "        r'\\\\subset': '⊂',\n",
    "        r'\\\\subseteq': '⊆',\n",
    "        r'\\\\cup': '∪',\n",
    "        r'\\\\cap': '∩',\n",
    "        r'\\\\implies': '⟹',\n",
    "        r'\\\\impliedby': '⟸',\n",
    "        r'\\\\to': '→',\n",
    "        r'\\\\longrightarrow': '⟶',\n",
    "        r'\\\\Rightarrow': '⇒',\n",
    "        r'\\\\Longrightarrow': '⟹',\n",
    "        r'\\\\propto': '∝',\n",
    "        r'\\\\bar': '¯',\n",
    "        r'\\\\tilde': '~',\n",
    "        r'\\\\breve': '˘',\n",
    "        r'\\\\hat': '^',\n",
    "        r'\\\\prime': '′',\n",
    "        r'\\\\dagger': '†',\n",
    "        r'\\\\ast': '∗',\n",
    "        r'\\\\star': '⋆',\n",
    "        r'\\\\cdots': '⋯',\n",
    "        r'\\\\vdots': '⋮',\n",
    "        r'\\\\ldots': '...',\n",
    "        \n",
    "        # Greek letters\n",
    "        r'\\\\alpha': 'α',\n",
    "        r'\\\\beta': 'β',\n",
    "        r'\\\\gamma': 'γ',\n",
    "        r'\\\\Gamma': 'Γ',\n",
    "        r'\\\\delta': 'δ',\n",
    "        r'\\\\Delta': 'Δ',\n",
    "        r'\\\\epsilon': 'ϵ',\n",
    "        r'\\\\varepsilon': 'ε',\n",
    "        r'\\\\zeta': 'ζ',\n",
    "        r'\\\\eta': 'η',\n",
    "        r'\\\\theta': 'θ',\n",
    "        r'\\\\Theta': 'Θ',\n",
    "        r'\\\\vartheta': 'ϑ',\n",
    "        r'\\\\iota': 'ι',\n",
    "        r'\\\\kappa': 'κ',\n",
    "        r'\\\\lambda': 'λ',\n",
    "        r'\\\\Lambda': 'Λ',\n",
    "        r'\\\\mu': 'μ',\n",
    "        r'\\\\nu': 'ν',\n",
    "        r'\\\\xi': 'ξ',\n",
    "        r'\\\\Xi': 'Ξ',\n",
    "        r'\\\\omicron': 'ο',\n",
    "        r'\\\\pi': 'π',\n",
    "        r'\\\\Pi': 'Π',\n",
    "        r'\\\\varpi': 'ϖ',\n",
    "        r'\\\\rho': 'ρ',\n",
    "        r'\\\\varrho': 'ϱ',\n",
    "        r'\\\\sigma': 'σ',\n",
    "        r'\\\\Sigma': 'Σ',\n",
    "        r'\\\\varsigma': 'ς',\n",
    "        r'\\\\tau': 'τ',\n",
    "        r'\\\\upsilon': 'υ',\n",
    "        r'\\\\Upsilon': 'Υ',\n",
    "        r'\\\\phi': 'ϕ',\n",
    "        r'\\\\Phi': 'Φ',\n",
    "        r'\\\\varphi': 'φ',\n",
    "        r'\\\\chi': 'χ',\n",
    "        r'\\\\psi': 'ψ',\n",
    "        r'\\\\Psi': 'Ψ',\n",
    "        r'\\\\omega': 'ω',\n",
    "        r'\\\\Omega': 'Ω',\n",
    "        \n",
    "        # Comparison operators\n",
    "        r'\\\\leq': '≤',\n",
    "        r'\\\\geq': '≥',\n",
    "        r'\\\\forall': '∀',\n",
    "        r'\\\\exists': '∃',\n",
    "        \n",
    "        # Space commands\n",
    "        r'\\\\quad': ' ',\n",
    "        r'\\\\qquad': '  ',\n",
    "        \n",
    "        # Algorithm related\n",
    "        r'\\\\operatorname\\{([^}]*)\\}': r'\\1',\n",
    "        \n",
    "        # Common substitutions\n",
    "        r'\\\\left\\(': '(',\n",
    "        r'\\\\right\\)': ')',\n",
    "        r'\\\\left\\[': '[',\n",
    "        r'\\\\right\\]': ']',\n",
    "        r'\\\\left\\{': '{',\n",
    "        r'\\\\right\\}': '}',\n",
    "        r'\\\\{': '{',\n",
    "        r'\\\\}': '}',\n",
    "        r'\\\\mid': '|',\n",
    "        r'_\\{([^}]*)\\}': r'_\\1',\n",
    "        r'\\^\\{([^}]*)\\}': r'^\\1',\n",
    "        \n",
    "        # Special formatting\n",
    "        r'\\\\begin\\{itemize\\}(.*?)\\\\end\\{itemize\\}': lambda match: '\\n' + '\\n'.join('- ' + item.strip() for item in re.split(r'\\\\item', match.group(1))[1:]) + '\\n',\n",
    "        r'\\\\begin\\{enumerate\\}(.*?)\\\\end\\{enumerate\\}': lambda match: '\\n' + '\\n'.join(f'{i+1}. ' + item.strip() for i, item in enumerate(re.split(r'\\\\item', match.group(1))[1:])) + '\\n',\n",
    "    }\n",
    "    \n",
    "    # Regular expression patterns for inline and display equations\n",
    "    inline_pattern = r'\\$([^\\$]+)\\$'\n",
    "    display_pattern = r'\\$\\$([^\\$]+)\\$\\$'\n",
    "    \n",
    "    # Process display equations first\n",
    "    display_matches = list(re.finditer(display_pattern, text))\n",
    "    for match in display_matches:\n",
    "        original_equation = match.group(0)\n",
    "        equation_content = match.group(1)\n",
    "        \n",
    "        # Apply conversions to the equation content\n",
    "        modified_content = equation_content\n",
    "        for latex_pattern, markdown_symbol in latex_to_markdown.items():\n",
    "            try:\n",
    "                if callable(markdown_symbol):\n",
    "                    modified_content = re.sub(latex_pattern, markdown_symbol, modified_content, flags=re.DOTALL)\n",
    "                else:\n",
    "                    modified_content = re.sub(latex_pattern, markdown_symbol, modified_content)\n",
    "            except re.error:\n",
    "                # Skip this pattern if it causes an error\n",
    "                print(f\"Warning: Skipping problematic pattern: {latex_pattern}\")\n",
    "                continue\n",
    "        \n",
    "        # Replace the original equation with the converted one\n",
    "        text = text.replace(original_equation, f\"\\n```math\\n{modified_content}\\n```\\n\")\n",
    "    \n",
    "    # Process inline equations\n",
    "    inline_matches = list(re.finditer(inline_pattern, text))\n",
    "    for match in inline_matches:\n",
    "        original_equation = match.group(0)\n",
    "        equation_content = match.group(1)\n",
    "        \n",
    "        # Apply conversions to the equation content\n",
    "        modified_content = equation_content\n",
    "        for latex_pattern, markdown_symbol in latex_to_markdown.items():\n",
    "            try:\n",
    "                if callable(markdown_symbol):\n",
    "                    modified_content = re.sub(latex_pattern, markdown_symbol, modified_content, flags=re.DOTALL)\n",
    "                else:\n",
    "                    modified_content = re.sub(latex_pattern, markdown_symbol, modified_content)\n",
    "            except re.error:\n",
    "                # Skip this pattern if it causes an error\n",
    "                print(f\"Warning: Skipping problematic pattern: {latex_pattern}\")\n",
    "                continue\n",
    "        \n",
    "        # Replace the original equation with the converted one\n",
    "        text = text.replace(original_equation, f\"`{modified_content}`\")\n",
    "    \n",
    "    # Additional post-processing\n",
    "    # Fix multiple blank lines\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    \n",
    "    # Preserve code blocks\n",
    "    code_blocks = re.findall(r'```.*?```', text, re.DOTALL)\n",
    "    for i, block in enumerate(code_blocks):\n",
    "        text = text.replace(block, f\"CODE_BLOCK_{i}\")\n",
    "    \n",
    "    # Clean up remaining LaTeX commands that weren't caught\n",
    "    text = re.sub(r'\\\\[a-zA-Z]+(\\{[^}]*\\})*', '', text)\n",
    "    \n",
    "    # Restore code blocks\n",
    "    for i, block in enumerate(code_blocks):\n",
    "        text = text.replace(f\"CODE_BLOCK_{i}\", block)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def process_markdown_file(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Process a markdown file and convert LaTeX notation to markdown equivalents.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(input_file, 'r', encoding='utf-8', errors='replace') as file:\n",
    "            content = file.read()\n",
    "        \n",
    "        # Convert LaTeX to markdown\n",
    "        converted_content = convert_latex_to_markdown(content)\n",
    "        \n",
    "        # Write converted content to output file\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            file.write(converted_content)\n",
    "        \n",
    "        print(f\"Successfully converted {input_file} to {output_file}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {input_file}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def process_directory(input_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Process all markdown files in the input directory and save converted files to the output directory.\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Get all markdown files in the input directory\n",
    "    input_path = Path(input_dir)\n",
    "    markdown_files = []\n",
    "    \n",
    "    # Search for markdown files recursively\n",
    "    for extension in ('*.md', '*.markdown'):\n",
    "        markdown_files.extend(list(input_path.glob(f\"**/{extension}\")))\n",
    "    \n",
    "    if not markdown_files:\n",
    "        print(f\"No markdown files found in {input_dir}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(markdown_files)} markdown files\")\n",
    "    \n",
    "    # Process each markdown file\n",
    "    successful_conversions = 0\n",
    "    for md_file in markdown_files:\n",
    "        try:\n",
    "            relative_path = md_file.relative_to(input_path)\n",
    "            output_file = Path(output_dir) / relative_path\n",
    "            \n",
    "            # Create any necessary subdirectories\n",
    "            output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            if process_markdown_file(md_file, output_file):\n",
    "                successful_conversions += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {md_file}: {str(e)}\")\n",
    "    \n",
    "    print(f\"Successfully converted {successful_conversions} out of {len(markdown_files)} files\")\n",
    "\n",
    "# Main function with hardcoded paths\n",
    "def main():\n",
    "    # Hardcoded input and output directories\n",
    "    input_dir = \"./content/pdf_content/ocr_output/BFS_notespdf\"\n",
    "    output_dir = \"./cleaned_md_outputs\"\n",
    "    \n",
    "    print(f\"Processing markdown files from {input_dir} to {output_dir}\")\n",
    "    process_directory(input_dir, output_dir)\n",
    "\n",
    "# Run the main function when the script is executed\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
