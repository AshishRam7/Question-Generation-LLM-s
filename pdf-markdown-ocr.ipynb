{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mistralai in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: eval-type-backport>=0.2.0 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from mistralai) (0.2.2)\n",
      "Requirement already satisfied: httpx>=0.27.0 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from mistralai) (0.28.1)\n",
      "Requirement already satisfied: jsonpath-python>=1.0.6 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from mistralai) (1.0.6)\n",
      "Requirement already satisfied: pydantic>=2.9.0 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from mistralai) (2.10.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from mistralai) (2.9.0.post0)\n",
      "Requirement already satisfied: typing-inspect>=0.9.0 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from mistralai) (0.9.0)\n",
      "Requirement already satisfied: anyio in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from httpx>=0.27.0->mistralai) (4.8.0)\n",
      "Requirement already satisfied: certifi in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from httpx>=0.27.0->mistralai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from httpx>=0.27.0->mistralai) (1.0.7)\n",
      "Requirement already satisfied: idna in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from httpx>=0.27.0->mistralai) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->mistralai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from pydantic>=2.9.0->mistralai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from pydantic>=2.9.0->mistralai) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from pydantic>=2.9.0->mistralai) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->mistralai) (1.17.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from typing-inspect>=0.9.0->mistralai) (1.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\questgen\\question-generation-llm-s\\.venv\\lib\\site-packages (from anyio->httpx>=0.27.0->mistralai) (1.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install mistralai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from mistralai import Mistral, DocumentURLChunk\n",
    "from mistralai.models import OCRResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded API Key: 9ABf...\n"
     ]
    }
   ],
   "source": [
    "# The only requirement for this script is to have a Mistral API Key.\n",
    "# You can get a free API Key at: https://console.mistral.ai/api-keys\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = \"9ABfnEldWDRL5oisq0gY53yvXujV94hX\"\n",
    "print(f\"Loaded API Key: {api_key[:4]}...\")\n",
    "client = Mistral(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path configuration\n",
    "INPUT_DIR = Path(\"./content/pdf_content/\")   # Folder where th9ABfnEldWDRL5oisq0gY53yvXujV94hXe user places the PDFs to be processed\n",
    "DONE_DIR = Path(\"./content/pdf_content/markdown_outputs\")            # Folder where processed PDFs will be moved\n",
    "OUTPUT_ROOT_DIR = Path(\"./content/pdf_content/ocr_output\")    # Root folder for conversion results\n",
    "\n",
    "# Ensure directories exist\n",
    "INPUT_DIR.mkdir(exist_ok=True)\n",
    "DONE_DIR.mkdir(exist_ok=True)\n",
    "OUTPUT_ROOT_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_images_in_markdown(markdown_str: str, images_dict: dict) -> str:\n",
    "    \"\"\"\n",
    "    This converts base64 encoded images directly in the markdown...\n",
    "    And replaces them with links to external images, so the markdown is more readable and organized.\n",
    "    \"\"\"\n",
    "    for img_name, base64_str in images_dict.items():\n",
    "        markdown_str = markdown_str.replace(f\"![{img_name}]({img_name})\", f\"![{img_name}]({base64_str})\")\n",
    "    return markdown_str\n",
    "\n",
    "def get_combined_markdown(ocr_response: OCRResponse) -> str:\n",
    "    \"\"\"\n",
    "    Part of the response from the Mistral API, which is an OCRResponse object...\n",
    "    And returns a single string with the combined markdown of all the pages of the PDF.\n",
    "    \"\"\"\n",
    "    markdowns: list[str] = []\n",
    "    for page in ocr_response.pages:\n",
    "        image_data = {}\n",
    "        for img in page.images:\n",
    "            image_data[img.id] = img.image_base64\n",
    "        markdowns.append(replace_images_in_markdown(page.markdown, image_data))\n",
    "\n",
    "    return \"\\n\\n\".join(markdowns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import base64\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Assuming these globals are defined elsewhere in your project:\n",
    "# OUTPUT_ROOT_DIR, DONE_DIR, client, DocumentURLChunk\n",
    "\n",
    "output_file_relative_path = None\n",
    "output_images_relative_path = None\n",
    "\n",
    "def process_pdf(pdf_path: Path):\n",
    "    global output_file_relative_path, output_images_relative_path\n",
    "    # Process all PDFs in INPUT_DIR\n",
    "    # - Important to be careful with the number of PDFs, as the Mistral API has a usage limit\n",
    "    #   and it could cause errors by exceeding the limit.\n",
    "\n",
    "    # PDF base name\n",
    "    pdf_base = pdf_path.stem\n",
    "    print(f\"Processing {pdf_path.name} ...\")\n",
    "    \n",
    "    # Output folders\n",
    "    output_dir = OUTPUT_ROOT_DIR / pdf_base\n",
    "    output_file_relative_path = output_dir\n",
    "    # Overwrite the output directory if it already exists\n",
    "    if output_dir.exists():\n",
    "        shutil.rmtree(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    images_dir = output_dir / \"images\"\n",
    "    output_images_relative_path = images_dir\n",
    "    images_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # PDF -> OCR\n",
    "    with open(pdf_path, \"rb\") as f:\n",
    "        pdf_bytes = f.read()\n",
    "        \n",
    "    uploaded_file = client.files.upload(\n",
    "        file={\n",
    "            \"file_name\": pdf_path.name,\n",
    "            \"content\": pdf_bytes,\n",
    "        },\n",
    "        purpose=\"ocr\"\n",
    "    )\n",
    "    \n",
    "    signed_url = client.files.get_signed_url(file_id=uploaded_file.id, expiry=1)\n",
    "    \n",
    "    ocr_response = client.ocr.process(\n",
    "        document=DocumentURLChunk(document_url=signed_url.url),\n",
    "        model=\"mistral-ocr-latest\",\n",
    "        include_image_base64=True\n",
    "    )\n",
    "    \n",
    "    # Save OCR in JSON \n",
    "    # (in case something fails it could be reused, but it is not used in the rest of the code)\n",
    "    ocr_json_path = output_dir / \"ocr_response.json\"\n",
    "    with open(ocr_json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(ocr_response.dict(), json_file, indent=4, ensure_ascii=False)\n",
    "    print(f\"OCR response saved in {ocr_json_path}\")\n",
    "    \n",
    "    # OCR -> Markdown prepared for Obsidian\n",
    "    # - That is, from base64 encoded images, it converts them to links to \n",
    "    #   external images and generates the images as such, in a subfolder.\n",
    "    \n",
    "    global_counter = 1\n",
    "    updated_markdown_pages = []\n",
    "    \n",
    "    for page in ocr_response.pages:\n",
    "        updated_markdown = page.markdown\n",
    "        for image_obj in page.images:\n",
    "            \n",
    "            # base64 to image\n",
    "            base64_str = image_obj.image_base64\n",
    "            if base64_str.startswith(\"data:\"):\n",
    "                base64_str = base64_str.split(\",\", 1)[1]\n",
    "            image_bytes = base64.b64decode(base64_str)\n",
    "            \n",
    "            # image extension handling\n",
    "            ext = Path(image_obj.id).suffix if Path(image_obj.id).suffix else \".png\"\n",
    "            new_image_name = f\"{pdf_base}_img_{global_counter}{ext}\"\n",
    "            global_counter += 1\n",
    "            \n",
    "            # save image in subfolder\n",
    "            image_output_path = images_dir / new_image_name\n",
    "            with open(image_output_path, \"wb\") as f:\n",
    "                f.write(image_bytes)\n",
    "            \n",
    "            # Update markdown with wikilink: ![[new_image_name]]\n",
    "            updated_markdown = updated_markdown.replace(\n",
    "                f\"![{image_obj.id}]({image_obj.id})\",\n",
    "                f\"![[{new_image_name}]]\"\n",
    "            )\n",
    "        updated_markdown_pages.append(updated_markdown)\n",
    "    \n",
    "    final_markdown = \"\\n\\n\".join(updated_markdown_pages)\n",
    "    output_markdown_path = output_dir / \"output.md\"\n",
    "    output_file_relative_path = output_markdown_path\n",
    "    with open(output_markdown_path, \"w\", encoding=\"utf-8\") as md_file:\n",
    "        md_file.write(final_markdown)\n",
    "    print(f\"Markdown generated in {output_markdown_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing BFS_notespdf.pdf ...\n",
      "OCR response saved in content\\pdf_content\\ocr_output\\BFS_notespdf\\ocr_response.json\n",
      "Markdown generated in content\\pdf_content\\ocr_output\\BFS_notespdf\\output.md\n",
      "BFS_notespdf.pdf moved to content\\pdf_content\\markdown_outputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anandaraman\\AppData\\Local\\Temp\\ipykernel_23456\\2116534252.py:60: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  json.dump(ocr_response.dict(), json_file, indent=4, ensure_ascii=False)\n"
     ]
    }
   ],
   "source": [
    "pdf_files = list(INPUT_DIR.glob(\"*.pdf\"))\n",
    "if not pdf_files:\n",
    "    print(\"No PDFs to process.\")\n",
    "    exit()\n",
    "    \n",
    "for pdf_file in pdf_files:\n",
    "    try:\n",
    "        process_pdf(pdf_file)\n",
    "        shutil.move(str(pdf_file), DONE_DIR / pdf_file.name)\n",
    "        print(f\"{pdf_file.name} moved to {DONE_DIR}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdf_file.name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(WindowsPath('content/pdf_content/ocr_output/BFS_notespdf/images'),\n",
       " WindowsPath('content/pdf_content/ocr_output/BFS_notespdf/output.md'))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_images_relative_path, output_file_relative_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No PDFs to process.\n"
     ]
    }
   ],
   "source": [
    "# Process all PDFs in INPUT_DIR\n",
    "# - Important to be careful with the number of PDFs, as the Mistral API has a usage limit\n",
    "#   and it could cause errors by exceeding the limit.\n",
    "\n",
    "pdf_files = list(INPUT_DIR.glob(\"*.pdf\"))\n",
    "if not pdf_files:\n",
    "    print(\"No PDFs to process.\")\n",
    "    exit()\n",
    "    \n",
    "for pdf_file in pdf_files:\n",
    "    try:\n",
    "        process_pdf(pdf_file)\n",
    "        shutil.move(str(pdf_file), DONE_DIR / pdf_file.name)\n",
    "        print(f\"{pdf_file.name} moved to {DONE_DIR}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdf_file.name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image: BFS_notespdf_img_1.jpeg\n",
      "Processing image: BFS_notespdf_img_2.jpeg\n",
      "Processing image: BFS_notespdf_img_3.jpeg\n",
      "Processing image: BFS_notespdf_img_4.jpeg\n",
      "Processing image: BFS_notespdf_img_5.jpeg\n",
      "Processing image: BFS_notespdf_img_6.jpeg\n",
      "Processing image: BFS_notespdf_img_7.jpeg\n",
      "Processing image: BFS_notespdf_img_8.jpeg\n",
      "Markdown file updated with image descriptions.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import moondream as md\n",
    "from PIL import Image\n",
    "\n",
    "# Initialize the Moondream model with your API key\n",
    "model = md.vl(api_key=\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJrZXlfaWQiOiJlODYyMDEzZC00NmVkLTRiNDYtOGMxZi0xYzYwMTUzY2M0YjkiLCJpYXQiOjE3Mzc1MjYyMjd9.0agZ8vgxwgrUJ7YMrIoBqGPs_4hsuh2zhqkwckxYkIM\")\n",
    "\n",
    "def generate_description_for_image(image_path, figure_caption=\"\"):\n",
    "    \"\"\"\n",
    "    Load an image from the provided path, encode it using the Moondream API,\n",
    "    and query for a description that is based on the provided figure caption.\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path)\n",
    "    encoded_image = model.encode_image(image)\n",
    "    query_text = (\n",
    "        f\"Describe the key technical findings in this figure/visualization \"\n",
    "        f\"captioned: {figure_caption} using natural language. Illustrate and mention trends, \"\n",
    "        f\"patterns, and numerical values that can be observed. Provide a scientific/academic styled short, \"\n",
    "        f\"single paragraph summary that is highly insightful in context of the document.\"\n",
    "    )\n",
    "    response = model.query(encoded_image, query_text)\n",
    "    description = response.get(\"answer\", \"No description available.\")\n",
    "    return description\n",
    "\n",
    "def extract_captions_from_markdown(markdown_path):\n",
    "    \"\"\"\n",
    "    Parse the markdown file to build a mapping from image filename to its figure caption.\n",
    "    It looks for placeholders like ![[filename]] and if the next line starts with \"Figure\",\n",
    "    uses that as the caption.\n",
    "    \"\"\"\n",
    "    captions = {}\n",
    "    with open(markdown_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        line = lines[i]\n",
    "        placeholder_match = re.search(r'!\\[\\[(.*?)\\]\\]', line)\n",
    "        if placeholder_match:\n",
    "            image_filename = placeholder_match.group(1)\n",
    "            caption = \"\"\n",
    "            # Check if the following line is a figure caption\n",
    "            if i + 1 < len(lines) and lines[i + 1].strip().startswith(\"Figure\"):\n",
    "                caption = lines[i + 1].strip()\n",
    "            captions[image_filename] = caption\n",
    "        i += 1\n",
    "    return captions\n",
    "\n",
    "def generate_image_descriptions(images_folder, captions_mapping):\n",
    "    \"\"\"\n",
    "    For each image file (as referenced by the markdown file), load the image from the\n",
    "    given images folder and generate a description using the Moondream API.\n",
    "    \"\"\"\n",
    "    descriptions = {}\n",
    "    for image_filename, caption in captions_mapping.items():\n",
    "        image_path = os.path.join(images_folder, image_filename)\n",
    "        if os.path.exists(image_path):\n",
    "            print(f\"Processing image: {image_filename}\")\n",
    "            description = generate_description_for_image(image_path, caption)\n",
    "        else:\n",
    "            description = \"Image file not found.\"\n",
    "        descriptions[image_filename] = description\n",
    "    return descriptions\n",
    "\n",
    "def update_markdown_file(markdown_path, image_descriptions):\n",
    "    \"\"\"\n",
    "    Update the markdown file by replacing the image placeholder (and the adjacent figure\n",
    "    caption) with a markdown block that includes the original caption and the generated\n",
    "    image description.\n",
    "    \"\"\"\n",
    "    with open(markdown_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    updated_lines = []\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        line = lines[i]\n",
    "        placeholder_match = re.search(r'!\\[\\[(.*?)\\]\\]', line)\n",
    "        if placeholder_match:\n",
    "            image_filename = placeholder_match.group(1)\n",
    "            caption = \"\"\n",
    "            # If the next line is a figure caption, capture it and skip it in the output.\n",
    "            if i + 1 < len(lines) and lines[i + 1].strip().startswith(\"Figure\"):\n",
    "                caption = lines[i + 1].strip()\n",
    "                i += 1  # Skip the caption line since we'll include it in our replacement.\n",
    "            description = image_descriptions.get(image_filename, \"No description available.\")\n",
    "            replacement = f\"{caption}\\n\\n**Image Description:** {description}\\n\"\n",
    "            updated_lines.append(replacement)\n",
    "        else:\n",
    "            updated_lines.append(line)\n",
    "        i += 1\n",
    "\n",
    "    with open(markdown_path, 'w', encoding='utf-8') as f:\n",
    "        f.writelines(updated_lines)\n",
    "\n",
    "def main():\n",
    "    # Define paths for images and markdown file.\n",
    "    global output_file_relative_path, output_images_relative_path\n",
    "    images_folder = str(output_images_relative_path)   # Update this path if your images are elsewhere.\n",
    "    markdown_file = str(output_file_relative_path) # Update this path if your markdown file is elsewhere.\n",
    "    \n",
    "    # First, extract the figure captions from the markdown file.\n",
    "    captions_mapping = extract_captions_from_markdown(markdown_file)\n",
    "    \n",
    "    # Next, generate image descriptions using the Moondream API.\n",
    "    image_descriptions = generate_image_descriptions(images_folder, captions_mapping)\n",
    "    \n",
    "    # Finally, update the markdown file by replacing placeholders with the descriptions.\n",
    "    update_markdown_file(markdown_file, image_descriptions)\n",
    "    \n",
    "    print(\"Markdown file updated with image descriptions.\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tool, initialize_agent\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Set OpenAI API Key\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from langchain.agents import Tool, initialize_agent\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Set OpenAI API Key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"  # Replace with your actual key\n",
    "global output_file_relative_path\n",
    "\n",
    "# Function to process LaTeX and non-markdown symbols\n",
    "def convert_markdown(md_content: str) -> str:\n",
    "    \"\"\"Cleans markdown content by replacing LaTeX expressions and non-markdown symbols.\"\"\"\n",
    "\n",
    "    # Normalize inline math expressions (ensure spaces inside `$...$` math expressions)\n",
    "    def fix_inline_math(match):\n",
    "        content = match.group(1).strip()\n",
    "        return f\"$ {content} $\"\n",
    "    \n",
    "    md_content = re.sub(r\"\\$(.+?)\\$\", fix_inline_math, md_content)\n",
    "    \n",
    "    # Replace common LaTeX symbols with markdown-friendly alternatives\n",
    "    latex_replacements = {\n",
    "        r\"\\Longrightarrow\": \"→\",\n",
    "        r\"\\Theta\": \"Θ\",\n",
    "        r\"\\cdot\": \"·\",\n",
    "        # Add more replacements if needed\n",
    "    }\n",
    "    \n",
    "    for latex_cmd, replacement in latex_replacements.items():\n",
    "        md_content = md_content.replace(latex_cmd, replacement)\n",
    "    \n",
    "    return md_content\n",
    "\n",
    "def parse_markdown_tool(input_text: str) -> str:\n",
    "    \"\"\"Tool that converts a given markdown text into a cleaned markdown format.\"\"\"\n",
    "    return convert_markdown(input_text)\n",
    "\n",
    "# Define the LangChain tool\n",
    "markdown_parser_tool = Tool(\n",
    "    name=\"MarkdownParser\",\n",
    "    func=parse_markdown_tool,\n",
    "    description=\"Scans markdown content for LaTeX or other non-markdown symbols and converts them into proper markdown format for hierarchical chunking.\"\n",
    ")\n",
    "\n",
    "# Initialize LangChain LLM agent\n",
    "llm = OpenAI(temperature=0)\n",
    "agent = initialize_agent(\n",
    "    tools=[markdown_parser_tool],\n",
    "    llm=llm,\n",
    "    agent=\"zero-shot-react-description\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Directory where the markdown files are stored\n",
    "input_directory = Path(str(output_file_relative_path))  # Change this to your actual directory\n",
    "\n",
    "# Process all markdown files in the directory and overwrite them\n",
    "for md_file in input_directory.glob(\"*.md\"):\n",
    "    with md_file.open(\"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # Use the agent to process the markdown content\n",
    "    converted_content = agent.run(content)\n",
    "    \n",
    "    # Overwrite the same file with the cleaned markdown content\n",
    "    with md_file.open(\"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(converted_content)\n",
    "    \n",
    "    print(f\"Processed and updated: {md_file.name}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
